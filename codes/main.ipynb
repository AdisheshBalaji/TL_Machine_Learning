{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olXDJi5WLtuh"
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import random\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "# Data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PIL and image handling\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import (\n",
    "    efficientnet_b0, EfficientNet_B0_Weights,\n",
    "    efficientnet_b3, EfficientNet_B3_Weights\n",
    ")\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering And Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pUPlacuQL2hP",
    "outputId": "6ff54cbd-585e-458a-8714-067cd4f44d32"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Defining paths \n",
    "base_path = \"../dataset_coin_classifications\"\n",
    "train_csv_path = \"../dataset_coin_classification/train.csv\"\n",
    "test_csv_path = \"../dataset_coin_classification/test.csv\"\n",
    "sample_submission_path = \"../dataset_coin_classification/sample_submission.csv\"\n",
    "\n",
    "# Define image directories\n",
    "train_img_dir = \"../dataset_coin_classification/train\"\n",
    "test_img_dir = \"../dataset_coin_classification/test\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Print information about the datasets\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter out invalid images\n",
    "def filter_valid_images(image_data, image_directory):\n",
    "    valid_images = []\n",
    "    \n",
    "    for _, row in tqdm(image_data.iterrows(), desc=\"Validating images\", total=len(image_data)):\n",
    "        image_filename = f\"{row['Id']}.jpg\"\n",
    "        image_path = os.path.join(image_directory, image_filename)\n",
    "        \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()  # Verifies if image is valid\n",
    "            valid_images.append(row)\n",
    "        except (UnidentifiedImageError, OSError, FileNotFoundError):\n",
    "            continue  # Skip invalid images\n",
    "    \n",
    "    return pd.DataFrame(valid_images).reset_index(drop=True)\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "train_df = filter_valid_images(train_df, train_img_dir)\n",
    "\n",
    "print(f\"Cleaned dataset size: {len(train_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the class column into numerical labels\n",
    "le = LabelEncoder()\n",
    "train_df['label'] = le.fit_transform(train_df['Class'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,                \n",
    "    test_size=0.2,           \n",
    "    random_state=42,         \n",
    "    stratify=train_df['Class']  \n",
    ")\n",
    "\n",
    "# Check the split\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fveAuLhxsaUf"
   },
   "outputs": [],
   "source": [
    "# CoinDataset class to handle image loading and transformations\n",
    "class CoinDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, raise_error=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.raise_error = raise_error\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.at[idx, 'Id']\n",
    "        img_name = f\"{img_id}.jpg\"\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            if self.raise_error:\n",
    "                raise FileNotFoundError(f\"[ERROR] Image not found: {img_path}\")\n",
    "            else:\n",
    "                print(f\"[WARNING] Image not found: {img_path} — using black image.\")\n",
    "                image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.df.at[idx, 'label']\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = train_df['Class'].unique()\n",
    "print(f'Number of classes: {num_classes}')\n",
    "\n",
    "class_counts = train_df['Class'].value_counts()\n",
    "class_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the Top 10 Classes\n",
    "Let's visualize the 10 most common classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = class_counts.head(10)\n",
    "plt.figure(figsize=(8, 4))\n",
    "top_10.plot(kind='bar')\n",
    "plt.title('Top 10 Classes by Image Count')\n",
    "plt.xlabel('Class Name')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Basic Data Augmentation before training on `ResNet18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTZHHq1Ysbjp"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Basic image transforms\n",
    "def get_transforms(img_size=224):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_transform\n",
    "\n",
    "# Create transforms\n",
    "train_transform, val_transform = get_transforms()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CoinDataset(train_df, train_img_dir, transform=train_transform)\n",
    "val_dataset = CoinDataset(val_df, train_img_dir, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers = 2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, num_workers = 2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def imshow(img, title):\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "# Function to run the training loop\n",
    "def train_model(model, train_loader, val_loader, \n",
    "                criterion, optimizer, scheduler = None, n_epochs=10, \n",
    "                device='cuda', model_name = \"ResNet_18\",\n",
    "                use_early_stopping=False, patience = 3,\n",
    "                use_scheduler=False, scheduler_step='epoch',\n",
    "                num_classes = 315, sample_classes_count = 10):\n",
    "    best_model_wts = None\n",
    "    best_acc = 0.0\n",
    "    early_stop_counter = 0\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "\n",
    "\n",
    "    # Arrays of training losses and accuracies for plotting\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "\n",
    "    sampled_classes = sorted(random.sample(range(num_classes), sample_classes_count))\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += torch.sum(preds == labels)\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train.double() / total_train\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += torch.sum(preds == labels)\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val.double() / total_val\n",
    "\n",
    "        \n",
    "        if use_scheduler and scheduler:\n",
    "            if scheduler_step == 'epoch':\n",
    "                scheduler.step()\n",
    "            elif scheduler_step == 'val_loss':\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        # Save the best model\n",
    "        if val_accuracy > best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            best_model_wts = model.state_dict()\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "\n",
    "        # Plot training and validation losses and accuracies\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_accuracy.item())\n",
    "        val_accuracies.append(val_accuracy.item())\n",
    "\n",
    "        # Print results for the current epoch\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        log_path = f\"../logs/log_{model_name}.txt\"\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        with redirect_stdout(f):\n",
    "\n",
    "            print(f\"Starting training for {model_name}\")\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                .\n",
    "                print(f\"Epoch {epoch+1}/{n_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "            print(f\"Training completed. Best Val Acc: {best_acc}\")\n",
    "\n",
    "        \n",
    "        \n",
    "        # Printing the f1-score, recall and precision after every epoch\n",
    "\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1-score: {f1:.4f}\")\n",
    "\n",
    "        # if use_early_stopping and early_stop_counter >= patience:\n",
    "        #     break\n",
    "\n",
    "        # Plot confusion matrix after every epoch (sampled classes)\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "        cm_sample = cm[np.ix_(sampled_classes, sampled_classes)]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm_sample, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=sampled_classes, yticklabels=sampled_classes)\n",
    "        plt.title(f'Confusion Matrix (Sample Classes) - Epoch {epoch+1}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"../plots/confusion_matrix_{epoch+1}_{model_name}.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Plot loss and accuracy graphs\n",
    "    epochs_range = range(n_epochs)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracies')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../plots/loss_accuracy_{model_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save the best model\n",
    "    torch.save(best_model_wts, f'../models/{model_name}.pth')\n",
    "\n",
    "    # Plotting the misclassified images after training\n",
    "    model.eval()\n",
    "    misclassified = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            mis = preds != labels\n",
    "            for img, pred, label, is_mis in zip(images, preds, labels, mis):\n",
    "                if is_mis:\n",
    "                    misclassified.append((img.cpu(), pred.item(), label.item()))\n",
    "                if len(misclassified) >= 8:\n",
    "                    break\n",
    "            if len(misclassified) >= 8:\n",
    "                break\n",
    "\n",
    "    # Show misclassified images\n",
    "    if misclassified:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, (img, pred, true) in enumerate(misclassified):\n",
    "            # Convert tensor to numpy if necessary\n",
    "            if hasattr(img, 'cpu'):\n",
    "                img_np = img.cpu().numpy()\n",
    "            else:\n",
    "                img_np = img\n",
    "\n",
    "            # If it's normalized , scale back to 0-255\n",
    "            if img_np.max() <= 1.0:\n",
    "                img_np = (img_np * 255).astype(np.uint8)\n",
    "            else:\n",
    "                img_np = img_np.astype(np.uint8)\n",
    "\n",
    "            # Convert to grayscale for analysis if it's RGB or has channels\n",
    "            if img_np.ndim == 3 and img_np.shape[0] in [1, 3]:  # Channels first (C, H, W)\n",
    "                img_np = img_np.transpose(1, 2, 0)  # To (H, W, C)\n",
    "            if img_np.ndim == 3:\n",
    "                gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_np\n",
    "\n",
    "            # Calculate mean and sta  dev\n",
    "            mean_intensity = np.mean(gray)\n",
    "            std_dev_intensity = np.std(gray)\n",
    "\n",
    "            # Print the stats\n",
    "            print(f\"Image {i+1} | Pred: {pred} | True: {true} | Mean: {mean_intensity:.2f} | Std Dev: {std_dev_intensity:.2f}\")\n",
    "\n",
    "            # Show image\n",
    "            plt.subplot(2, 4, i+1)\n",
    "            plt.imshow(gray, cmap='gray')\n",
    "            plt.title(f\"Pred: {pred}\\nTrue: {true}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(\"Misclassified Images\", fontsize=16, y=1.05)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Frozen `ResNet18` with parameters\n",
    "- Number Of Epochs: 10 \n",
    "- Loss Function: Cross Entropy Loss\n",
    "- Optimizer: Adam with learning rate $10^{-4}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all layers initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze 'layer4' block and 'fc'\n",
    "for name, param in model.namedParameters():\n",
    "    if 'layer4' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "# Defining final layer\n",
    "model.fc = nn.Linear(model.fc.in_features, 315)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training basic ResNet_18 model, we will use 10 eopochs as default\n",
    "n_epochs = 10\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=n_epochs,\n",
    "    device=device,\n",
    "    model_name=\"ResNet_18\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOQNqa26JWUD"
   },
   "source": [
    "### Observation\n",
    " Freezing the **earlier layers** still leads to decent accuracies, like around 82 % with our `ResNet18` model since the earlier layers are good at detecting low-level features such as edges and shapes. The final layer `model.fc` captures complex patters, hence this `ResNet18` CNN model can be generalized to many tasks.\n",
    "\n",
    " ### Advantages\n",
    " **Less Parameter Update**: Since you're freezing most of the model's parameters, the number of parameters being updated during training is significantly reduced. This leads to faster computation in each iteration because fewer gradients need to be calculated and applied.\n",
    "\n",
    "**Lower Memory Usage**: Fewer parameters to update means less memory usage during training, which can improve training speed, especially if you're working with limited GPU memory(colab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPU6KJTuL3yh"
   },
   "source": [
    "### Fine-Tuning with `ResNet18`\n",
    "We now **unfreeze** all the layers of the model and restart training with same hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pretrained resnet18 with all layers unfrozen\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the final classification layer for 315 classes\n",
    "model.fc = nn.Linear(model.fc.in_features, 315)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the fully unfrozen model\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=n_epochs,\n",
    "    device=device,\n",
    "    model_name = \"ResNet18_fine_tune\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Model Performance Comparison\n",
    "\n",
    "We evaluated the performance of two configurations of the ResNet18 model: one with the convolutional base **frozen**, and the other with it **unfrozen** (i.e., full fine-tuning). The results are summarized below:\n",
    "\n",
    "* **Frozen CNN**\n",
    "\n",
    "  * **Training Accuracy**: 93.36%\n",
    "  * **Validation Accuracy**: 75.86%\n",
    "\n",
    "* **Unfrozen CNN**\n",
    "\n",
    "  * **Training Accuracy**: 96.28%\n",
    "  * **Validation Accuracy**: 80.33%\n",
    "\n",
    "Both of the Models show clear **overfitting** since there is a huge difference between the training and validation accuracies\n",
    "\n",
    "Interestingly, the **training time for both configurations was nearly identical**, with the frozen model completing in **4 minutes 15 seconds**, and the unfrozen model in **4 minutes 22 seconds**. This minimal time difference is somewhat unexpected, as training a frozen network involves updating significantly fewer parameters compared to a fully trainable one.\n",
    "\n",
    "However, the accuracy difference aligns with expectations. The unfrozen model benefits from full gradient flow through all layers, allowing it to adapt more specifically to the dataset. This flexibility leads to improved performance on both the training and validation sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning\n",
    "We are running experiments to evaluate how key hyperparameters affect model performance. Specifically, we vary:\n",
    "\n",
    "Learning Rate: 1e-3, 1e-4\n",
    "\n",
    "Batch Size: 64, 128\n",
    "\n",
    "Fully Connected Hidden Units: 256, 1024\n",
    "\n",
    "Dropout Rate: 0.2, 0.5\n",
    "\n",
    "Each combination is trained for 10 epochs using the same training and validation sets. The goal is to identify a balanced model in terms of training speed, accuracy, and generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoO_JWFDWW-r"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Defining experiment configurations\n",
    "experiments = [\n",
    "    {\"lr\": 1e-4, \"batch_size\": 64, \"hidden_units\": 256, \"dropout\": 0.2},\n",
    "    {\"lr\": 1e-4, \"batch_size\": 64, \"hidden_units\": 1024, \"dropout\": 0.5},\n",
    "    {\"lr\": 1e-3, \"batch_size\": 128, \"hidden_units\": 256, \"dropout\": 0.5},\n",
    "]\n",
    "\n",
    "\n",
    "# Running a train loop on each experiment\n",
    "for i, config in enumerate(experiments):\n",
    "    log_path = f\"../logs/log_resnet18_exp{i+1}.txt\"\n",
    "    with open(log_path, \"w\") as f:\n",
    "        with redirect_stdout(f):\n",
    "            print(f\"\\n=== Experiment {i+1} ===\")\n",
    "            print(f\"LR: {config['lr']}, Batch Size: {config['batch_size']}, Hidden Units: {config['hidden_units']}, Dropout: {config['dropout']}\")\n",
    "\n",
    "            # Reload dataloaders\n",
    "            train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], num_workers=2, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], num_workers=2, shuffle=False)\n",
    "\n",
    "            # Load ResNet18\n",
    "            model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "            # Custom FC head\n",
    "            model.fc = nn.Sequential(\n",
    "                nn.Linear(model.fc.in_features, config['hidden_units']),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(config['dropout']),\n",
    "                nn.Linear(config['hidden_units'], 315)\n",
    "            )\n",
    "\n",
    "            model = model.to(device)\n",
    "\n",
    "            # Loss and optimizer\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config['lr'])    \n",
    "\n",
    "            # Train\n",
    "            train_model(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                n_epochs=10,\n",
    "                model_name=f\"resnet18_exp{i+1}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experiment Summary\n",
    "\n",
    "### Experiment Results\n",
    "\n",
    "* **Learning Rate**: 1e-4\n",
    "* **Batch Size**: 64\n",
    "* **Hidden Units**: 1024\n",
    "* **Dropout**: 0.5\n",
    "\n",
    "### Performance:\n",
    "\n",
    "* **Best Performing Experiment**:\n",
    "\n",
    "  * **Train Accuracy**: 86.66%\n",
    "  * **Val Accuracy**: 83.25%\n",
    "  * **Precision**: 0.8355\n",
    "  * **Recall**: 0.8169\n",
    "  * **F1-score**: 0.8112\n",
    "* This configuration shows the best balance between training and validation accuracy, with the lowest gap between the two, suggesting better generalization.\n",
    "\n",
    "### Observations:\n",
    "\n",
    "* The model with a **learning rate of 1e-4**, **dropout of 0.5**, and **1024 hidden units** worked best in preventing overfitting while maintaining good performance on the validation set.\n",
    "* The model with **learning rate 1e-3** and **batch size 128** showed signs of **divergence** (very high loss and low accuracy), which suggests the learning rate might have been too high for the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high difference between training accuracy and validation accuracy in most of the models indicates **overfitting**. </br>\n",
    "Let's try\n",
    "- Early Stopping\n",
    "- Data augmentation\n",
    "- Label Smoothing\n",
    "- Scheduler\n",
    "- L2 regularization with momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Techniques\n",
    "\n",
    "To improve model generalization and robustness, especially under real-world variations, the following data augmentation techniques were applied to the training images:\n",
    "\n",
    "1. **Random Occlusion**  \n",
    "   - Randomly applies 1 or more square patches of fixed size (`16x16`) with a gray color `(128, 128, 128)` to simulate missing or blocked parts of the image.\n",
    "   - This helps the model learn to focus on more global features and become resilient to partial obstructions.\n",
    "\n",
    "2. **Random Resized Crop**  \n",
    "   - Randomly crops a region of the image and resizes it to the target size (`224x224`), with scale varying between 80% and 100% of the original image size.\n",
    "   - Helps the model deal with variations in object scale and location.\n",
    "\n",
    "3. **Random Horizontal Flip**  \n",
    "   - Flips the image horizontally with a 50% chance.\n",
    "   - Introduces left-right invariance in the model.\n",
    "\n",
    "4. **Color Jitter**  \n",
    "   - Randomly changes brightness, contrast, saturation, and hue of the image within a small range.\n",
    "   - Simulates changes in lighting and color conditions.\n",
    "\n",
    "5. **Random Rotation**  \n",
    "   - Rotates the image randomly within ±15 degrees.\n",
    "   - Makes the model invariant to small orientation changes.\n",
    "\n",
    "6. **Random Affine Transform (Shear & Scale)**  \n",
    "   - Applies random shear transformations (up to 10 degrees) and scales the image between 80% and 120% of its original size.\n",
    "   - Adds robustness to geometric distortions and aspect ratio changes.\n",
    "\n",
    "7. **Normalization**  \n",
    "   - Normalizes the image tensor using ImageNet mean and standard deviation:\n",
    "     - Mean: `[0.485, 0.456, 0.406]`\n",
    "     - Std: `[0.229, 0.224, 0.225]`\n",
    "   - Ensures consistent input distribution for the model.\n",
    "\n",
    "These augmentations are applied only to training images. Validation images are simply resized and normalized without augmentation to ensure consistent evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Occlusion class that applies random occlusions to images\n",
    "class RandomOcclusion:\n",
    "    def __init__(self, occlusion_size=16, max_occlusions=1, occlusion_color=(128, 128, 128)):\n",
    "        self.occlusion_size = occlusion_size\n",
    "        self.max_occlusions = max_occlusions\n",
    "        self.occlusion_color = occlusion_color\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        num_occlusions = random.randint(1, self.max_occlusions)\n",
    "        img = img.copy()\n",
    "        \n",
    "        for _ in range(num_occlusions):\n",
    "            top = random.randint(0, h - self.occlusion_size)\n",
    "            left = random.randint(0, w - self.occlusion_size)\n",
    "            occlusion = Image.new('RGB', (self.occlusion_size, self.occlusion_size), self.occlusion_color)\n",
    "            img.paste(occlusion, (left, top))\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "# Adjusted transformations with more cautious parameters\n",
    "def get_transforms(img_size=224, use_occlusion=True):\n",
    "    train_transform_list = [\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    "    if use_occlusion:\n",
    "        train_transform_list.insert(0, RandomOcclusion())  # Add occlusion only for training\n",
    "\n",
    "    train_transform = transforms.Compose(train_transform_list)\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "\n",
    "train_transform, val_transform = get_transforms()\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CoinDataset(train_df, train_img_dir, transform=train_transform)\n",
    "val_dataset = CoinDataset(val_df, train_img_dir, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, num_workers=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code to check the data augmentations made on the images\n",
    "img_path = \"../dataset_coin_classification/train/8.jpg\"\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Define transformation (with occlusion)\n",
    "train_transform, _ = get_transforms()\n",
    "\n",
    "# Apply the transformation\n",
    "img_transformed = train_transform(img)\n",
    "\n",
    "# Convert the tensor back to an image for visualization\n",
    "img_transformed = img_transformed.permute(1, 2, 0).numpy()  # Convert from (C, H, W) to (H, W, C)\n",
    "\n",
    "# PLot\n",
    "plt.imshow(img_transformed)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in val_loader: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Final Model after tuning parameters\n",
    "\n",
    "- **Architecture**: ResNet18 with pretrained weights, freezing layers up to `layer4` for faster training.\n",
    "- **Custom Classifier**: Replaces the final layer with a 512-neuron hidden layer and 315 output classes.\n",
    "- **Loss Function**: CrossEntropyLoss.\n",
    "- **Optimizer**: AdamW with a learning rate of `5e-4` and weight decay `1e-4`.\n",
    "- **Scheduler**: CosineAnnealingLR with `T_max=15`.\n",
    "- **Mixed Precision**: Uses `GradScaler` for faster training.\n",
    "- **Training**: 15 epochs with early stopping and learning rate scheduler enabled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "# Load ResNet18 with pretrained weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze early layers to speed up training and reduce overfitting\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" not in name:  # Only keep last block trainable\n",
    "        param.requires_grad = False\n",
    "\n",
    "# (fully connected head)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, 315) \n",
    ") \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer with decoupled weight decay\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler for quick convergence\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "# Mixed precision scaler for faster training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    n_epochs=15,  \n",
    "    device=device,\n",
    "    model_name=\"resnet18_custom\",\n",
    "    use_early_stopping=True,\n",
    "    use_scheduler=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load EfficientNet-B0 with pretrained weights\n",
    "model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "# Unfreeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace the classifier head\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.classifier[1].in_features, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 315)  # 315 classes\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
    "\n",
    "# Train\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    n_epochs=15,\n",
    "    device=device,\n",
    "    model_name=\"efficientnet_b0_custom\",\n",
    "    use_early_stopping=True,\n",
    "    use_scheduler=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "\n",
    "I think we will move on to training on the test data, I am experiencing a bottleneck at 83-84% accuracy, we will use the `resnet18_custom.pth` file to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping functions\n",
    "def create_class_to_idx(train_df):\n",
    "    return dict(train_df[['Class', 'label']].drop_duplicates().values)\n",
    "\n",
    "def create_idx_to_class(train_df):\n",
    "    return dict(train_df[['label', 'Class']].drop_duplicates().values)\n",
    "\n",
    "class_to_idx = create_class_to_idx(train_df)\n",
    "idx_to_class = create_idx_to_class(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset class\n",
    "class CoinDatasetTest(Dataset):\n",
    "    # Initialization method\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # Method to get an item\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx]['Id']\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, img_id\n",
    "        except (FileNotFoundError, UnidentifiedImageError):\n",
    "            return None, img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Custom collate_fn to handle missing images (None)\n",
    "def collate_fn(batch):\n",
    "    images, img_ids = zip(*batch)\n",
    "    \n",
    "    # Filter out None images and corresponding img_ids\n",
    "    valid_images = [img for img in images if img is not None]\n",
    "    valid_ids = [img_id for img, img_id in zip(images, img_ids) if img is not None]\n",
    "    \n",
    "    # Pad with 'Unknown' class if necessary\n",
    "    predictions = []\n",
    "    for img, img_id in zip(images, img_ids):\n",
    "        if img is None:\n",
    "            predictions.append({\n",
    "                \"Id\": int(img_id),\n",
    "                \"Class\": 'Unknown',\n",
    "                \"label\": -1  # Represent missing class with -1\n",
    "            })\n",
    "    \n",
    "    # Stack valid images into a tensor\n",
    "    if valid_images:\n",
    "        valid_images = torch.stack(valid_images)\n",
    "    else:\n",
    "        valid_images = torch.tensor([])  # Empty tensor if no valid images\n",
    "    \n",
    "    return valid_images, valid_ids, predictions\n",
    "\n",
    "\n",
    "# Dataset & Dataloader for Test Data with custom collate_fn\n",
    "test_dataset = CoinDatasetTest(test_df, test_img_dir, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Load model and modify the last layer (same as during training)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(512, 315)  # Ensure this matches the number of classes in training\n",
    ")\n",
    "model.load_state_dict(torch.load(\"../models/resnet18_custom.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Predict on the test data and handle missing images\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, img_ids, missing_preds in tqdm(test_loader):\n",
    "        # Process missing predictions\n",
    "        predictions.extend(missing_preds)\n",
    "        \n",
    "        if images.numel() > 0:  # Only process if there are valid images\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            preds = preds.cpu().numpy()\n",
    "\n",
    "            for img_id, pred in zip(img_ids, preds):\n",
    "                predictions.append({\n",
    "                    \"Id\": int(img_id),\n",
    "                    \"Class\": idx_to_class[pred],  # Map predicted label to class name\n",
    "                    \"label\": int(pred)\n",
    "                })\n",
    "\n",
    "# Convert predictions to a DataFrame\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Ensure the predictions are sorted by ID as required\n",
    "submission_df = submission_df.sort_values(\"Id\").reset_index(drop=True)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the label column from the submission DataFrame\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "submission = submission.drop(columns=['label'])\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
